{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetCDF to Cloud Optimized GeoTIFF (COG) Transformation\n",
    "\n",
    "This notebook demonstrates how to transform NetCDF files into Cloud Optimized GeoTIFFs for ingestion into the GHG Center.\n",
    "\n",
    "**Dataset Example**: CASA-GFED3 Land Carbon Flux  \n",
    "**Source Format**: NetCDF  \n",
    "**Target Format**: Cloud Optimized GeoTIFF (COG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the ingestion configuration including S3 buckets and transformation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\": \"ghgc-data-store-dev\",\n",
    "    \"raw_data_prefix\": \"raw_data/casa-gfed/\",\n",
    "    \"cog_data_bucket\": \"ghgc-data-store-dev\",\n",
    "    \"cog_data_prefix\": \"transformed_cogs/casa-gfed-v3\",\n",
    "    \"date_fmt\": \"%Y%m\",\n",
    "    \"transformation\": {\n",
    "        \"reproject_to\": \"EPSG:4326\",\n",
    "        \"compression\": \"DEFLATE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize S3 client\n",
    "session = boto3.session.Session()\n",
    "s3_client = session.client(\"s3\")\n",
    "bucket_name = config[\"cog_data_bucket\"]\n",
    "date_fmt = config[\"date_fmt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Process\n",
    "\n",
    "Process each NetCDF file:\n",
    "1. Open the dataset\n",
    "2. Fix coordinate issues (longitude wrap)\n",
    "3. Extract variables\n",
    "4. Create COGs for each time step and variable\n",
    "5. Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track processed files\n",
    "files_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\n",
    "\n",
    "# Process NetCDF files\n",
    "for name in os.listdir(\"geoscarb\"):\n",
    "    # Open NetCDF dataset\n",
    "    xds = xarray.open_dataset(\n",
    "        f\"geoscarb/{name}\",\n",
    "        engine=\"netcdf4\",\n",
    "    )\n",
    "    \n",
    "    # Fix longitude coordinates (wrap from 0-360 to -180-180)\n",
    "    xds = xds.assign_coords(\n",
    "        longitude=(((xds.longitude + 180) % 360) - 180)\n",
    "    ).sortby(\"longitude\")\n",
    "    \n",
    "    # Get list of data variables\n",
    "    variable = [var for var in xds.data_vars]\n",
    "\n",
    "    # Process each time step\n",
    "    for time_increment in range(0, len(xds.time)):\n",
    "        # Process each variable\n",
    "        for var in variable[:-1]:\n",
    "            filename = name.split(\"/ \")[-1]\n",
    "            filename_elements = re.split(\"[_ .]\", filename)\n",
    "            \n",
    "            # Extract data for this time step and variable\n",
    "            data = getattr(xds.isel(time=time_increment), var)\n",
    "            \n",
    "            # Flip latitude to match expected orientation\n",
    "            data = data.isel(latitude=slice(None, None, -1))\n",
    "            \n",
    "            # Set spatial dimensions and CRS\n",
    "            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n",
    "            data.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "            # Format date for filename\n",
    "            date = data.time.dt.strftime(date_fmt).item(0)\n",
    "            \n",
    "            # Create COG filename\n",
    "            filename_elements.pop()  # Remove extension\n",
    "            filename_elements[-1] = date  # Replace with formatted date\n",
    "            filename_elements.insert(2, var)  # Insert variable name\n",
    "            cog_filename = \"_\".join(filename_elements)\n",
    "            cog_filename = f\"{cog_filename}.tif\"\n",
    "\n",
    "            # Write COG to temporary file and upload to S3\n",
    "            with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                data.rio.to_raster(\n",
    "                    temp_file.name,\n",
    "                    driver=\"COG\",\n",
    "                )\n",
    "                s3_client.upload_file(\n",
    "                    Filename=temp_file.name,\n",
    "                    Bucket=bucket_name,\n",
    "                    Key=f\"{config['cog_data_prefix']}/{cog_filename}\",\n",
    "                )\n",
    "\n",
    "            # Track processed files\n",
    "            files_processed = files_processed._append(\n",
    "                {\"file_name\": name, \"COGs_created\": cog_filename},\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "            print(f\"Generated and saved COG: {cog_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metadata\n",
    "\n",
    "Extract and save metadata from the NetCDF files for the STAC collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata to S3\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n",
    "    metadata = {\n",
    "        \"attributes\": xds.attrs,\n",
    "        \"data_dimensions\": dict(xds.dims),\n",
    "        \"data_variables\": list(xds.data_vars),\n",
    "        \"spatial_extent\": {\n",
    "            \"xmin\": float(xds.longitude.min()),\n",
    "            \"xmax\": float(xds.longitude.max()),\n",
    "            \"ymin\": float(xds.latitude.min()),\n",
    "            \"ymax\": float(xds.latitude.max())\n",
    "        },\n",
    "        \"temporal_extent\": {\n",
    "            \"start\": str(xds.time.min().values),\n",
    "            \"end\": str(xds.time.max().values)\n",
    "        }\n",
    "    }\n",
    "    json.dump(metadata, fp, indent=2)\n",
    "    fp.flush()\n",
    "\n",
    "    s3_client.upload_file(\n",
    "        Filename=fp.name,\n",
    "        Bucket=bucket_name,\n",
    "        Key=f\"{config['cog_data_prefix']}/metadata.json\",\n",
    "    )\n",
    "\n",
    "# Save conversion log\n",
    "files_processed.to_csv(\n",
    "    f\"s3://{bucket_name}/{config['cog_data_prefix']}/files_converted.csv\",\n",
    ")\n",
    "\n",
    "print(\"Done generating COGs\")\n",
    "print(f\"Total files processed: {len(files_processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Considerations\n",
    "\n",
    "### 1. Coordinate Handling\n",
    "- NetCDF files may use different longitude conventions (0-360 vs -180-180)\n",
    "- Always verify and transform to EPSG:4326 for consistency\n",
    "\n",
    "### 2. Variable Selection\n",
    "- Process each variable as a separate asset\n",
    "- Skip auxiliary variables (e.g., time_bounds)\n",
    "\n",
    "### 3. Temporal Processing\n",
    "- Extract individual time steps for temporal datasets\n",
    "- Use consistent date formatting in filenames\n",
    "\n",
    "### 4. COG Generation\n",
    "- Use appropriate compression (DEFLATE for general use)\n",
    "- Consider predictor settings for specific data types\n",
    "\n",
    "### 5. Metadata Preservation\n",
    "- Extract and save original NetCDF attributes\n",
    "- Document spatial and temporal extents\n",
    "- Track all processed files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}